{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.special\n",
    "from scipy.stats import ortho_group\n",
    "from scipy import optimize\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from state_evolution.algorithms.state_evolution import StateEvolution # Standard SP iteration\n",
    "from state_evolution.data_models.custom import Custom # Custom data model. You input the covariances\n",
    "from state_evolution.experiments.learning_curve import CustomExperiment\n",
    "from state_evolution.data_models.custom import CustomSpectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_raw_data(n,d,F_t,theta):\n",
    "    np.random.seed(n)\n",
    "    inputs=np.random.randn(d,n)\n",
    "    hidden=np.tanh(F_t@inputs/np.sqrt(d))\n",
    "    #hidden=F_t@inputs/np.sqrt(d)\n",
    "    labels=theta@hidden\n",
    "    return inputs.T, labels\n",
    "\n",
    "def give_dataset(n_test,n_train,d,F_t,theta):\n",
    "    x_test,y_test=give_raw_data(n_test,d,F_t,theta)\n",
    "    x_train,y_train=give_raw_data(n_train,d,F_t,theta)\n",
    "    \n",
    "    training_data=CustomDataset(x_train,y_train)\n",
    "    testing_data=CustomDataset(x_test,y_test)\n",
    "    \n",
    "    return DataLoader(training_data, batch_size=int(n_train), shuffle=True), DataLoader(testing_data, batch_size=int(n_test), shuffle=True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.labels = labels\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        Input = self.inputs[idx]\n",
    "        Label = self.labels[idx]\n",
    "        return Input, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f30ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "COEFICIENTS = {'relu': (1/np.sqrt(2*np.pi), 0.5, np.sqrt((np.pi-2)/(4*np.pi))), \n",
    "               'erf': (0, 2/np.sqrt(3*np.pi), 0.200364), 'tanh': (0, 0.605706, 0.165576),\n",
    "               'sign': (0, np.sqrt(2/np.pi), np.sqrt(1-2/np.pi))}\n",
    "\n",
    "# Coefficients\n",
    "_, kappa1_teacher, kappastar_teacher = COEFICIENTS['tanh']\n",
    "_, kappa1_student, kappastar_student = COEFICIENTS['tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d7aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def g3m_input_matrices_simple(d,F_s,F_t):\n",
    "    # Covariances\n",
    "    Psi = (kappa1_teacher**2 * F_t @ F_t.T)/d + kappastar_teacher**2 * np.identity(d)\n",
    "    Omega = (kappa1_student**2 * F_s @ F_s.T)/d + kappastar_student**2 * np.identity(d)\n",
    "    Phi = (kappa1_teacher * kappa1_student * F_t @ F_s.T)/d + kappastar_student*kappastar_teacher * np.identity(d)\n",
    "    return Psi,Omega,Phi\n",
    "\n",
    "\n",
    "def g3m_input_matrices_MC(d,n,F_s,F_t):\n",
    "    x=np.random.randn(d,n)\n",
    "    u=np.tanh(F_t@x/np.sqrt(d))\n",
    "    v=np.tanh(F_s@x/np.sqrt(d))\n",
    "    Psi=1/n*(u@u.T)\n",
    "    Omega=1/n*(v@v.T)\n",
    "    Phi=1/n*(u@v.T)\n",
    "    return Psi,Omega,Phi\n",
    "\n",
    "\n",
    "def g3m_prediction(d, n_list, reg, F_s, F_t, theta, method):\n",
    "    n_covariance=10000\n",
    "    if method=='MC':\n",
    "        Psi,Omega,Phi=g3m_input_matrices_MC(d,n_covariance, F_s, F_t)\n",
    "    if method=='simple':\n",
    "        Psi,Omega,Phi=g3m_input_matrices_simple(d,F_s, F_t)\n",
    "    \n",
    "    data_model = Custom(teacher_teacher_cov = Psi, \n",
    "                        student_student_cov = Omega, \n",
    "                        teacher_student_cov = Phi,\n",
    "                        teacher_weights = theta)\n",
    "    \n",
    "    my_experiment = CustomExperiment(task = 'ridge_regression', \n",
    "                                     regularisation = reg, \n",
    "                                     data_model = data_model, \n",
    "                                     initialisation='uninformed', \n",
    "                                     tolerance = 1e-15,\n",
    "                                     damping = 0.1, \n",
    "                                     verbose = False, \n",
    "                                     max_steps = 5000)\n",
    "    my_experiment.learning_curve(alphas = n_list/d)\n",
    "    a=my_experiment.get_curve()\n",
    "    Eg_g3m=np.array(a['test_error'])\n",
    "    Et_g3m=np.array(a['train_loss'])\n",
    "    L_g3m=np.array(a['loss'])\n",
    "    return Eg_g3m,Et_g3m,L_g3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12755e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, criterion, optimizer, d, regularisation, verbose=True, info='train_loss',train=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = y.size(dim=0)*criterion(pred[:,0]/np.sqrt(d), y/np.sqrt(d))\n",
    "        for i,param in enumerate(model.parameters()):\n",
    "            if i == 1:            #only regularize the last layer\n",
    "                loss += regularisation *torch.norm(param)**2\n",
    "        if train:\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            if info=='train_loss':\n",
    "                return criterion(pred[:,0]/np.sqrt(d), y/np.sqrt(d)).item()\n",
    "            if info=='loss':\n",
    "                return loss.item()/d\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, criterion, optimizer, d, verbose=True):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    test_loss_bis = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += criterion(pred[:,0]/np.sqrt(d), y/np.sqrt(d)).item()\n",
    "            test_loss_bis += np.mean(((pred[:,0]/np.sqrt(d) - y/np.sqrt(d))**2).numpy())\n",
    "            if np.abs(test_loss - test_loss_bis) > 1e-10:\n",
    "                print(test_loss - test_loss_bis)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if verbose:\n",
    "        print(f\"generalisation error: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a92524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d,Lambda,alpha,init_type,epochs,freeze_1st_layer,F_t,theta,F_random,theta_random):    \n",
    "    Loss_decay=[]\n",
    "    Eg=[]\n",
    "    Et=[]\n",
    "    Loss=[]\n",
    "    W=[]\n",
    "    Fs=[]\n",
    "    n_array=(d*np.array(alpha)).astype(int)\n",
    "\n",
    "    n_test=50000 #number of samples used to evaluate the generalisation error\n",
    "    for reg in Lambda:\n",
    "        fs=[]\n",
    "        w=[]\n",
    "        loss_decay=[]\n",
    "        eg,et,l=[],[],[]\n",
    "        for j,n_train in enumerate(n_array):\n",
    "\n",
    "            #Building the network\n",
    "            model=nn.Sequential(nn.Linear(d, d, bias=False),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(d, 1, bias=False))\n",
    "\n",
    "            #Initialisation\n",
    "            if init_type=='random':\n",
    "                model[0].weight.data=torch.from_numpy(F_random/np.sqrt(d))\n",
    "                model[2].weight.data=torch.from_numpy(np.array([theta_random]))\n",
    "            if init_type=='planted':\n",
    "                model[0].weight.data=torch.from_numpy(F_t/np.sqrt(d))\n",
    "                model[2].weight.data=torch.from_numpy(np.array([theta]))\n",
    "\n",
    "            model = model.double()\n",
    "\n",
    "            # Define the loss\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Optimizers require the parameters to optimize and a learning rate\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            #Freeze first layer\n",
    "            if freeze_1st_layer:\n",
    "                for i,param in enumerate(model.parameters()):\n",
    "                    if i==0:\n",
    "                        param.requires_grad = False\n",
    "\n",
    "            loss=[]\n",
    "            train_dataloader,test_dataloader = give_dataset(n_test,n_train,d,F_t,theta)\n",
    "            for t in range(epochs):\n",
    "                loss.append(train_loop(train_dataloader,model,criterion,optimizer,d,reg,verbose=True,info='loss',train=True))\n",
    "                if t == epochs-1:\n",
    "                    eg.append(test_loop(test_dataloader,model,criterion,optimizer,d,verbose=False))\n",
    "                    et.append(train_loop(train_dataloader,model,criterion,optimizer,d,reg,verbose=True,train=True))\n",
    "                    l.append(train_loop(train_dataloader,model,criterion,optimizer,d,reg,verbose=True,info='loss',train=True))\n",
    "                    fs.append(copy.deepcopy((model[0].weight.data.numpy())*np.sqrt(d)))\n",
    "                    w.append(copy.deepcopy(model[2].weight.data.numpy()))\n",
    "            loss_decay.append(loss)\n",
    "            print(f\"lambda={reg}, alpha={alpha[j]} is done\")\n",
    "        Eg.append(eg)\n",
    "        Et.append(et)\n",
    "        Loss.append(l)\n",
    "        W.append(w)\n",
    "        Fs.append(fs)\n",
    "        Loss_decay.append(loss_decay)\n",
    "    return Eg,Et,Loss,W,Fs,Loss_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a062ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict = {'red':   ((0.0,  1.0, 1.0),\n",
    "                   (0.5,  0.6, 0.6),\n",
    "                   (1.0,  0.5, 0.5)),\n",
    "\n",
    "         'green': ((0.0,  0.3, 0.3),\n",
    "                   (0.5,  0.4, 0.4),\n",
    "                   (1.0,  0.5, 0.5)),\n",
    "\n",
    "         'blue':  ((0.0,  0.3, 0.3),\n",
    "                   (0.5,  0.6, 0.6),\n",
    "                   (1.0,  1.0, 1.0))}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
